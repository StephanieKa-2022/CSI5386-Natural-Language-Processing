{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FkBHW-SdrfeJ",
        "vSGgdzn9JI2g",
        "PvvQUZ9UIKPh",
        "Cp9Pk9n7lelZ",
        "IQBW7VvJbrSi",
        "Yz-p9Kb_XWMK"
      ],
      "authorship_tag": "ABX9TyPpZmKVjCWhQDjObtIULaWD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yyyhpets2022/CSI5386-Natural-Language-Processing/blob/main/CSI_5386_NPL_Assignment_1_Part_2_%7C_S_Bert_%26_USE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions before running the program:\n",
        "1.   Add the text file in your program (if needed)\n",
        "2.   Copy the name of the file\n",
        "3.   Please update the name under which the CSV will be saved in each section \n",
        "\n",
        "Description:\n",
        "*   The following program implement 4 sentence embedding models: two Sentence-BERT models and two Universal Sentence Encoder models\n",
        "*   Each models results will be saved as CSV containing 4 columns: Sentence 1, Sentence 2, and two sets of similarity scores"
      ],
      "metadata": {
        "id": "cbo1fLoYHYsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assign the text file name to text_name between the quotation marks\n",
        "text_name = \"STS2016.input.answer-answer.txt\"\n",
        "\n",
        "texts = []\n",
        "\n",
        "with open (text_name, 'rt') as myfile:  \n",
        "    for myline in myfile:\n",
        "      texts.append(myline)\n",
        "\n",
        "sentence_list = [x for i in texts for x in i.split(\"\\t\")]\n",
        "\n",
        "out = [sentence_list[i: i+4] for i in range(0, len(sentence_list), 4)]\n",
        "\n",
        "df = pd.DataFrame(out, columns = ['Sentence 1', 'Sentence 2', 'Link 1', \"Link 2\"])\n",
        "del df['Link 1']\n",
        "del df['Link 2']\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cCdaXYx5SOLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence-BERT\n",
        "# *Model : all-MiniLM-L6-v2*"
      ],
      "metadata": {
        "id": "FkBHW-SdrfeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Sentence-BERT"
      ],
      "metadata": {
        "id": "VyDaLPdTrfeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "YlOoQNqwrfeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sbert = df.copy()"
      ],
      "metadata": {
        "id": "aB2KtN-mrfeW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model for S-BERT used in the implementation:\n",
        "*   all-MiniLM-L6-v2\n"
      ],
      "metadata": {
        "id": "ERPmbvQErfeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util \n",
        "# The different models for S-BERT:\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\") \n",
        "\n",
        "# Getting the columns containing the sentences into a list \n",
        "SentencesSbert_1 = df_sbert[\"Sentence 1\"].tolist()\n",
        "SentencesSbert_2 = df_sbert[\"Sentence 2\"].tolist()\n",
        "\n",
        "# Computing the embeddings for the sentences \n",
        "EmbeddingsSbert_1 = model.encode(SentencesSbert_1, convert_to_tensor=True)\n",
        "EmbeddingsSbert_2 = model.encode(SentencesSbert_2, convert_to_tensor=True)\n",
        "\n",
        "# Calculting the cosine similarity of each sentences' pair, and multiple the similarity score by 5\n",
        "cosine_scores_sbert_sentences = util.cos_sim(EmbeddingsSbert_1, EmbeddingsSbert_2)"
      ],
      "metadata": {
        "id": "3sIVePi0rfeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Score_Sbert = []\n",
        "\n",
        "for e in range(len(SentencesSbert_1)):\n",
        "  Score_Sbert.append(float(\"{:.4f}\".format(cosine_scores_sbert_sentences[e][e])))\n",
        "\n",
        "Score_Sbert_5 = [x * 5 for x in Score_Sbert]\n",
        "\n",
        "#  Adding the S-BERT scores to the dataframe  \n",
        "df_sbert.insert(loc = 2,\n",
        "          column=\"Score S-BERT w/ L6-v2\",\n",
        "          value=Score_Sbert)\n",
        "\n",
        "#  Adding the Score_Sbert_5 to the dataframe\n",
        "df_sbert.insert(loc=3,\n",
        "              column=\"Score S-BERT times 5\",\n",
        "              value=Score_Sbert_5)"
      ],
      "metadata": {
        "id": "XX7lJ-XarfeX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sbert.head()"
      ],
      "metadata": {
        "id": "T2ByW512rfeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbert_max_L6_v2 = df_sbert['Score S-BERT w/ L6-v2'].values == df_sbert['Score S-BERT w/ L6-v2'].max()\n",
        "df_sbert.loc[sbert_max_L6_v2]"
      ],
      "metadata": {
        "id": "OOF6xiKB0mv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSV creation:\n",
        "This is where you can update the CSV name"
      ],
      "metadata": {
        "id": "FMF8Pp-gN9Ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_name_L6_V2 = r'all-MiniLM-L6-v2.csv'\n",
        "df_sbert.to_csv(csv_name_L6_V2, index = False)"
      ],
      "metadata": {
        "id": "jIV8CgsHrfeX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence-BERT\n",
        "# *Model : all-mpnet-base-v2*"
      ],
      "metadata": {
        "id": "vSGgdzn9JI2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Sentence-BERT"
      ],
      "metadata": {
        "id": "Ve0NWMDbil_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "gT1THBpP-JW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sbert = df.copy()"
      ],
      "metadata": {
        "id": "Q8qkGbe00wLW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model for S-BERT used in the implementation:\n",
        "*   all-mpnet-base-v2\n",
        "\n"
      ],
      "metadata": {
        "id": "2MF35NtgQ9fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util \n",
        "# The different models for S-BERT:\n",
        "model = SentenceTransformer(\"all-mpnet-base-v2\") \n",
        "\n",
        "# Getting the columns containing the sentences into a list \n",
        "SentencesSbert_1 = df_sbert[\"Sentence 1\"].tolist()\n",
        "SentencesSbert_2 = df_sbert[\"Sentence 2\"].tolist()\n",
        "\n",
        "# Computing the embeddings for the sentences \n",
        "EmbeddingsSbert_1 = model.encode(SentencesSbert_1, convert_to_tensor=True)\n",
        "EmbeddingsSbert_2 = model.encode(SentencesSbert_2, convert_to_tensor=True)\n",
        "\n",
        "# Calculting the cosine similarity of each sentences' pair, and multiple the similarity score by 5\n",
        "cosine_scores_sbert_sentences = util.cos_sim(EmbeddingsSbert_1, EmbeddingsSbert_2)"
      ],
      "metadata": {
        "id": "gFDiGiDL3RVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Score_Sbert = []\n",
        "\n",
        "for e in range(len(SentencesSbert_1)):\n",
        "  Score_Sbert.append(float(\"{:.4f}\".format(cosine_scores_sbert_sentences[e][e])))\n",
        "\n",
        "Score_Sbert_5 = [x * 5 for x in Score_Sbert]\n",
        "\n",
        "#  Adding the S-BERT scores to the dataframe  \n",
        "df_sbert.insert(loc = 2,\n",
        "          column=\"Score S-BERT w/ base-v2\",\n",
        "          value=Score_Sbert)\n",
        "\n",
        "#  Adding the Score_Sbert_5 to the dataframe\n",
        "df_sbert.insert(loc=3,\n",
        "              column=\"Score S-BERT times 5\",\n",
        "              value=Score_Sbert_5)"
      ],
      "metadata": {
        "id": "8uaVf-Qz9t7u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sbert.head()"
      ],
      "metadata": {
        "id": "-1nmn_G8EjLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbert_max_base_v2 = df_sbert['Score S-BERT w/ base-v2'].values == df_sbert['Score S-BERT w/ base-v2'].max()\n",
        "df_sbert.loc[sbert_max_base_v2]"
      ],
      "metadata": {
        "id": "rKTOWEdS0DXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSV creation: This is where you can update the CSV name"
      ],
      "metadata": {
        "id": "CEe_Ww2NOYh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_name_based_V2 = r'all-mpnet-base-v2.csv'\n",
        "df_sbert.to_csv(csv_name_based_V2, index = False)"
      ],
      "metadata": {
        "id": "Ij4SCmsYyIkl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Universal Sentence Encoder\n",
        "# *Model: universal-sentence-encoder/4*"
      ],
      "metadata": {
        "id": "PvvQUZ9UIKPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade tensorflow-gpu\n",
        "!pip3 install tensorflow-hub"
      ],
      "metadata": {
        "id": "W8d9XNAjyD5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Kb0c7nYQLNQs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The USE model used in the implementation link:\n",
        "*   https://tfhub.dev/google/universal-sentence-encoder/4\n",
        "\n"
      ],
      "metadata": {
        "id": "WMzv90rQRxGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model : universal-sentence-encoder/4\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n",
        "print(\"module %s loaded\" % module_url)"
      ],
      "metadata": {
        "id": "ywfIZNpMMNBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_use = df.copy()\n",
        "df_use.head()"
      ],
      "metadata": {
        "id": "ZGAxeOlKKiui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the sentences from the dataframe and transfer it to a list \n",
        "SentencesUse_1 = df_use[\"Sentence 1\"].tolist()\n",
        "SentencesUse_2 = df_use[\"Sentence 2\"].tolist()\n",
        "\n",
        "# Calculating the embeddings for the sentences \n",
        "EmbeddingsUse_1 = model(SentencesUse_1)\n",
        "EmbeddingsUse_2 = model(SentencesUse_2)"
      ],
      "metadata": {
        "id": "59fcDfv8wL8N"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity scores using the Universal Sentence Encoder \n",
        "\n",
        "#  Creating a list first \n",
        "Score_Use = []\n",
        "\n",
        "#  Calculating the cosine similarity scores \n",
        "for index in range(len(SentencesUse_1)):\n",
        "  dot_product = np.dot(EmbeddingsUse_1[index], EmbeddingsUse_2[index])\n",
        "  mag_vect1 = np.sqrt(np.dot(EmbeddingsUse_1[index], EmbeddingsUse_1[index]))\n",
        "  mag_vect2 = np.sqrt(np.dot(EmbeddingsUse_2[index], EmbeddingsUse_2[index]))\n",
        "  cos_sim_ang = dot_product / (mag_vect1 * mag_vect2)\n",
        "  Score_Use.append(cos_sim_ang)\n",
        "\n",
        "#  Adding the USE scores to the dataframe  \n",
        "df_use.insert(loc=2,\n",
        "              column=\"Score USE w/ Encoder 4\",\n",
        "              value=Score_Use)\n",
        "\n",
        "#  Converting the USE scores by multiplying it by 5\n",
        "df_use.insert(loc=3,\n",
        "              column=\"Score USE times 5\",\n",
        "              value=5*df_use[\"Score USE w/ Encoder 4\"]\n",
        ")"
      ],
      "metadata": {
        "id": "23E_C-YhyuVR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_use.head()"
      ],
      "metadata": {
        "id": "1V6o_rVO4Bir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_use[\"Score USE w/ Encoder 4\"].max())\n",
        "print(df_use[\"Score USE w/ Encoder 4\"].min())"
      ],
      "metadata": {
        "id": "TaXEuI0u3tjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_use[\"Score USE times 5\"].max())\n",
        "print(df_use[\"Score USE times 5\"].min())"
      ],
      "metadata": {
        "id": "nbNV21WO7R5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_max_encoder5 = df_use['Score USE w/ Encoder 4'].values == df_use['Score USE w/ Encoder 4'].max()\n",
        "df_use.loc[use_max_encoder5]"
      ],
      "metadata": {
        "id": "I9dmpvzlzieC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PvnG8CkpOv2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSV creation: This is where you can update the CSV name"
      ],
      "metadata": {
        "id": "GHnZm13rOz0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_name_enc_4 = r'universal-sentence-encoder-4.csv'\n",
        "df_use.to_csv(csv_name_enc_4, index = False)"
      ],
      "metadata": {
        "id": "kh5yAsMB9FPs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Universal Sentence Encoder \n",
        "# *Model: universal-sentence-encoder-large/5*"
      ],
      "metadata": {
        "id": "Cp9Pk9n7lelZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade tensorflow-gpu\n",
        "!pip3 install tensorflow-hub"
      ],
      "metadata": {
        "id": "R6iQ5NOYlell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ddJ6MVmWlell"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The USE model used in the implementation link:\n",
        "*   https://tfhub.dev/google/universal-sentence-encoder-large/5\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7CYvTk2lell"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model : universal-sentence-encoder-large/5\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
        "model = hub.load(module_url)\n",
        "print(\"module %s loaded\" % module_url)"
      ],
      "metadata": {
        "id": "4MZ61hE4lell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_use = df.copy()\n",
        "df_use.head()"
      ],
      "metadata": {
        "id": "nSQzdKXnlelm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the sentences from the dataframe and transfer it to a list \n",
        "SentencesUse_1 = df_use[\"Sentence 1\"].tolist()\n",
        "SentencesUse_2 = df_use[\"Sentence 2\"].tolist()\n",
        "\n",
        "# Calculating the embeddings for the sentences \n",
        "EmbeddingsUse_1 = model(SentencesUse_1)\n",
        "EmbeddingsUse_2 = model(SentencesUse_2)"
      ],
      "metadata": {
        "id": "N1zb8DC1lelm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity scores using the Universal Sentence Encoder \n",
        "\n",
        "#  Creating a list first \n",
        "Score_Use = []\n",
        "\n",
        "#  Calculating the cosine similarity scores \n",
        "for index in range(len(SentencesUse_1)):\n",
        "  dot_product = np.dot(EmbeddingsUse_1[index], EmbeddingsUse_2[index])\n",
        "  mag_vect1 = np.sqrt(np.dot(EmbeddingsUse_1[index], EmbeddingsUse_1[index]))\n",
        "  mag_vect2 = np.sqrt(np.dot(EmbeddingsUse_2[index], EmbeddingsUse_2[index]))\n",
        "  cos_sim_ang = dot_product / (mag_vect1 * mag_vect2)\n",
        "  Score_Use.append(cos_sim_ang)\n",
        "\n",
        "#  Adding the USE scores to the dataframe  \n",
        "df_use.insert(loc=2,\n",
        "              column=\"Score USE w/ Encoder Large 5\",\n",
        "              value=Score_Use)\n",
        "\n",
        "#  Converting the USE scores by multiplying it by 5\n",
        "df_use.insert(loc=3,\n",
        "              column=\"Score USE times 5\",\n",
        "              value=5*df_use[\"Score USE w/ Encoder Large 5\"]\n",
        ")"
      ],
      "metadata": {
        "id": "0Qz4sS-llelm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_use.head()"
      ],
      "metadata": {
        "id": "K4UYWUqDlelm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_use[\"Score USE w/ Encoder Large 5\"].max())\n",
        "print(df_use[\"Score USE w/ Encoder Large 5\"].min())"
      ],
      "metadata": {
        "id": "4Y859INZlelq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_use[\"Score USE times 5\"].max())\n",
        "print(df_use[\"Score USE times 5\"].min())"
      ],
      "metadata": {
        "id": "9wE3c93blelq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_max_large5 = df_use['Score USE w/ Encoder Large 5'].values == df_use['Score USE w/ Encoder Large 5'].max()\n",
        "df_use.loc[use_max_large5]"
      ],
      "metadata": {
        "id": "7OtPad0_yrEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSV creation: This is where you can update the CSV name"
      ],
      "metadata": {
        "id": "8v6CWtbEPDEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_name_enc_5 = r'universal-sentence-encoder-large-5.csv'\n",
        "df_use.to_csv(csv_name_enc_5, index = False)"
      ],
      "metadata": {
        "id": "sx4vaXAnlelq"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}